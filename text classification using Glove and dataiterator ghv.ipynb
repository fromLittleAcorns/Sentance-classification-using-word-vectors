{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Notebook to look at a how to use a word vector approach to catagorise sentances.\n",
    "\n",
    "This is to help with analysing and assessing transaction data from a company based upon the \n",
    "description of the transaction.\n",
    "\n",
    "The approach is to use a pre-trained set of word vectors - in this case the GloVe set 6.B with each vector being of \n",
    "length 300.\n",
    "\n",
    "In this analysis the approach is to define a fixed sentance length and pad sentances where they are less than this.\n",
    "\n",
    "I have tried lengths of 5 to 10, and 6 or 7 seems to work best. At present I am padding the sentances in my routine, \n",
    "I intend to see how well the padding in the pytorch embedding layer works and whether this is any better since\n",
    "I am not sure if the padding is having a detrimental effect upon the analysis.\n",
    "\n",
    "After the embedding layer I am using a simple 3 level neural network, the first two layers with rectified linear and \n",
    "then finally a softmax output\n",
    "\n",
    "The previous analysis achieved accuracy of 88.9% on training data but 66.2% on the test data using a simple \n",
    "bag of words approach.\n",
    "\n",
    "The present analysis gives me 93% on the training data and 95% on the test data, hence it is a big improvement in the analysis of the untrained datasets.  More could be done but the actual data itself needs some work and some of the catagories are not very well represented.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from myData import Dataset\n",
    "from myData import DataLoader\n",
    "#from torchtext import data as t_data\n",
    "#from torchtext import utils\n",
    "from torchtext.vocab import load_word_vectors\n",
    "import numpy as np\n",
    "#import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import word_processing as wp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# System parameters\n",
    "my_file_path='johnrichmond/Dropbox/Machine Learning/text classification/Andrew/'\n",
    "csv_file_name='Payment items.csv'\n",
    "\n",
    "stop_list= set(\"for a c e do h i if is it in g o p or r t u v y 's ' of the and mr ms to nd we\".\n",
    "               split())\n",
    "pad='<pad>'\n",
    "\n",
    "remove_single_words=True\n",
    "max_sent_length=7\n",
    "min_freq=1\n",
    "use_subset_data=True\n",
    "max_cases=30000\n",
    "\n",
    "# only there are catagories 1-14 are valid, all others should be rejected\n",
    "\n",
    "min_cat=0\n",
    "max_cat=13\n",
    "num_cat=max_cat+1\n",
    "# Note actual catagory labels have been removed for reasons of commercial sensitivity\n",
    "label_to_idx={\"Cat 0\":0, \n",
    "              \"Cat 1\": 1,\n",
    "              \"Cat 2\":2,\n",
    "              \"Cat 3\": 3,\n",
    "              \"Cat 4\": 4,\n",
    "              \"Cat 5\":5,\n",
    "              \"Cat 6\": 6,\n",
    "              \"Cat 7\":7,\n",
    "              \"Cat 8\":8,\n",
    "              \"Cat 9\":9,\n",
    "              \"Cat 10\":10,\n",
    "              \"Cat 11\":11,\n",
    "              \"Cat 12\":12,\n",
    "              \"Cat 13\":13\n",
    "             }\n",
    "load_word_vector_set='glove.6B'\n",
    "word_vector_length=300\n",
    "word_vector_path='johnrichmond/Dropbox/Machine Learning/text classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Analysis Hyper parameters\n",
    "batch_size=32\n",
    "no_epochs=800\n",
    "# Define size of first hidden layer\n",
    "HL1_size=300\n",
    "# Define size of second hidden layer\n",
    "HL2_size=120\n",
    "# Define size of third hidden layer\n",
    "HL3_size=50\n",
    "# Define learning rate\n",
    "lr=0.006\n",
    "# Define momentum term used in stochastic gradient descent\n",
    "momentum=0.2\n",
    "nesterov=False\n",
    "# Define weight decay term\n",
    "L2=0.000\n",
    "# Define percantage of data to use for validation\n",
    "val_percentage=20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the file paths the same whether using Mac or Linux\n",
    "if sys.platform == 'darwin':\n",
    "    start='/Users/'\n",
    "else: start='/home/'\n",
    "    \n",
    "file_name=my_file_path+csv_file_name\n",
    "txt_file=start+file_name\n",
    "word_vec_path=os.path.join(start,word_vector_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replace_list={\"years\":\"year\", \"yr\":\"year\", \"wks\":\"week\",\"tickets\": \"ticket\",\n",
    "              \"terms\":\"term\", \"students\":\"student\",\"pupils\":\"pupil\",\"meals\": \"meal\",\n",
    "              \"lakes\":\"lake\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to measure classification accuracy - move to module once programme \n",
    "# is operating\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    # The next line is not needed in this case since it is done prior to the call\n",
    "    #_, pred = output.topk(maxk, 1, True, True) # topk is torch function to return highest values in array\n",
    "    pred = output.t()  #Transpose\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    \"\"\"\n",
    "    Note - the expand is a torch command to expend one tensor to the size of another\n",
    "    target os a one D tensor. target.view(1,-1) reshapes the tensor.  The -1 means this \n",
    "    is chosen by the software to get the right total size.  The first 1 indicates the number \n",
    "    of rows to use.\n",
    "    The net outcome is an array with one column of length maxk for each target value.  \n",
    "    The entire column is filled with the target value to facilitate easy comparison. \n",
    "    The correct array then contains an array with true wherever the tar\n",
    "    \"\"\"\n",
    "    #get value matches the prediction\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)    # The nomeclature[:k] returns the top k rows Since\n",
    "                                                           # there is no second array we get the every column.\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentances to catagorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(txt_file,'rU') as file_obj:\n",
    "    f_data=[]\n",
    "    num=0\n",
    "    lines=[]\n",
    "    reader=csv.reader(file_obj)\n",
    "    for line in reader:\n",
    "        if reader.line_num<>1:\n",
    "    #       with col in line:\n",
    "            text_str=line[0]\n",
    "            catagory=line[15]\n",
    "            if wp.is_integer(catagory):\n",
    "                f_data.append([text_str,int(catagory)])\n",
    "                num=num+1\n",
    "        if reader.line_num>max_cases and use_subset_data: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All data loaded, will now process each line of data\n",
    "# Initially remove numbers and punctuation\n",
    "sentances=[]\n",
    "catagories=[]\n",
    "catagory=[]\n",
    "identity=np.identity(num_cat)\n",
    "for row in f_data:\n",
    "    row[0]=wp.clean_str(row[0])\n",
    "    row[0]=wp.rem_numbers(row[0])# Done separately since I might not always want to do this\n",
    "\n",
    "    if row[0] in (None, \"\"):\n",
    "        # row rejected\n",
    "        continue\n",
    "    elif row[1] <1 or row[1]> num_cat or row[1] in (None, \"\"):\n",
    "        continue\n",
    "    sentance=row[0].split(\" \")\n",
    "    # remove stop list words\n",
    "    sentance=wp.remove_stop_words(sentance,stop_list)\n",
    "    sentance=wp.replace_similar_words(sentance,replace_list)\n",
    "    if len(sentance)==0: continue\n",
    "    row[1]=row[1]-1   \n",
    "    \n",
    "    # Getting to this point implies the row is ok and still has valid words, therefore will add\n",
    "    sentances.append(sentance)\n",
    "    catagories.append(identity[row[1]-1,:])\n",
    "    catagory.append(row[1])\n",
    "    \n",
    "# Remove single words  \n",
    "if remove_single_words==True:\n",
    "    final_sentances=[]\n",
    "    final_catagories=[]\n",
    "    final_catagory=[]\n",
    "    word_counts = Counter(itertools.chain(*sentances))\n",
    "    new_sentances=[[word for word in sentance if word_counts[word]>1] \n",
    "                    for sentance in sentances]\n",
    "    #Remove empty entries from both sentances and catagories\n",
    "    for index,sentance in enumerate(new_sentances):\n",
    "        if len(sentance)<>0:\n",
    "            final_sentances.append(sentance)\n",
    "            final_catagories.append(catagories[index])\n",
    "            final_catagory.append(catagory[index])\n",
    "    sentances=final_sentances\n",
    "    catagories=final_catagories\n",
    "    catagory=final_catagory\n",
    "    \n",
    "# Identify the longest sentance\n",
    "max_length=0\n",
    "for sentance in sentances:\n",
    "    if len(sentance)> max_length: max_length=len(sentance)\n",
    "print \"Maximum sentance length: {} words\".format(max_length)\n",
    "\n",
    "# Pad sentances to maximum length\n",
    "if max_sent_length<>0:\n",
    "    pad_to=max_sent_length\n",
    "else:\n",
    "    pad_to=max_length\n",
    "for sentance in sentances:\n",
    "    len_sent=len(sentance)\n",
    "    if len_sent<pad_to:\n",
    "        for pos in range(len_sent,pad_to):\n",
    "            sentance.append(pad)\n",
    "\n",
    "for i in range(len(sentances)):\n",
    "    sentances[i]=sentances[i][0:max_sent_length]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make use of the use of the pytext Vocab class \n",
    "from torchtext import vocab\n",
    "test=Counter(itertools.chain(*sentances))\n",
    "v=vocab.Vocab(test,wv_type=load_word_vector_set,wv_dim=300, unk_init='random', specials=[pad], min_freq=min_freq)\n",
    "\n",
    "# We now have v.itos which containes an ordered list of words\n",
    "# v.stoi which is a disctionary that links a word to an index\n",
    "# The number of words is given by len(v.itos)\n",
    "vocab_size=len(v.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now need to shuffle the input data\n",
    "from random import shuffle\n",
    "no_sentances=len(catagory)\n",
    "# Create index and shuffle\n",
    "shuffle_idx=[i for i in range(no_sentances)]\n",
    "shuffle(shuffle_idx)\n",
    "new_sentances=[]\n",
    "new_catagory=[]\n",
    "for idx in shuffle_idx:\n",
    "    new_sentances.append(sentances[idx])\n",
    "    new_catagory.append(catagory[idx])\n",
    "# for now will save the originals but do not need to in future\n",
    "sentances_bak=sentances[:]\n",
    "catagory_bak=catagory[:]\n",
    "sentances=new_sentances\n",
    "catagory=new_catagory\n",
    "\n",
    "# The lists are now shuffled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# will setup simple batches and sets for now\n",
    "total_sentances=len(sentances)\n",
    "total_batches=int(total_sentances/batch_size)\n",
    "train_batches=int(total_batches*0.8)\n",
    "val_batches=total_batches-train_batches\n",
    "\n",
    "print \"Total valid sentances: {:d}\".format(total_sentances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class sent_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    This is a very simplified version of a Pytorch dataset that I am using with my own \n",
    "    bespoke versions of the Pytorch dataiterator since I could not get the proper versions\n",
    "    to work and this seemed the easiest way.  It does not support multi workers, transforms etc\n",
    "    \n",
    "    Args:\n",
    "        sentances: expected to be a list of sentnaces, each containing a list of words \n",
    "                    as strings\n",
    "        catagory: the corresponding catagory number of each sentance\n",
    "        word_to_idx: a dictionary linking a word to an index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentances, catagory, word_to_idx):\n",
    "\n",
    "        self.sentances = sentances\n",
    "        self.catagory = catagory\n",
    "        self.word_to_idx = word_to_idx\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            context_idxs: a list of the word indexes in the sentance corresponding to the \n",
    "            sentance index\n",
    "            catagory[index]: the catagory of the sentance\n",
    "        \"\"\"\n",
    "        context_idxs=(map(lambda w: self.word_to_idx[w], sentances[index]))\n",
    "             \n",
    "        #print \"sentance nums= \",sentance\n",
    "\n",
    "        return context_idxs, catagory[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.catagory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds=sent_dataset(sentances[0:train_batches*batch_size],\n",
    "                      catagory[0:train_batches*batch_size],\n",
    "                      v.stoi)\n",
    "val_ds=sent_dataset(sentances[train_batches*batch_size:total_batches*batch_size],\n",
    "                      catagory[train_batches*batch_size:total_batches*batch_size],\n",
    "                      v.stoi)\n",
    "train_loader=DataLoader(dataset=train_ds,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True)\n",
    "val_loader=DataLoader(dataset=val_ds,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextClassifier_simple(nn.Module):\n",
    "    def __init__(self, catagories, vocab_size, sent_length,embedding_dim, HL1_size, HL2_size):\n",
    "        super(TextClassifier_simple, self).__init__()\n",
    "        self.embeddings=nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1=nn.Linear(embedding_dim*sent_length,HL1_size)\n",
    "        self.dropout=nn.Dropout()\n",
    "        self.linear2=nn.Linear(HL1_size, HL2_size)\n",
    "        self.linear3=nn.Linear(HL2_size, catagories)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #embeds=self.embeddings(inputs).view(1,-1)\n",
    "        embeds=self.embeddings(inputs).view(len(inputs),-1)\n",
    "        out=F.relu(self.linear1(embeds))\n",
    "        out=self.dropout(out)\n",
    "        out=F.relu(self.linear2(out))\n",
    "        out=self.linear3(out)\n",
    "        log_probs=F.log_softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define classifier, optimiser and prevent modification of the word vector weights\n",
    "losses=[]\n",
    "loss_function=nn.NLLLoss()\n",
    "model=TextClassifier_simple(num_cat, vocab_size, pad_to, word_vector_length, HL1_size, \n",
    "                            HL2_size)\n",
    "optimizer=optim.SGD(model.parameters(),lr, weight_decay=L2, momentum=momentum, \n",
    "                    nesterov=nesterov)\n",
    "model.embeddings.weight.data.copy_(v.vectors)\n",
    "model.embeddings.weight.requires_grad = False\n",
    "loss_function.parameters = filter(lambda p: p.requires_grad, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ntopk=3\n",
    "predict=torch.LongTensor(train_batches*batch_size,ntopk).zero_()\n",
    "all_targets=torch.LongTensor(train_batches*batch_size).zero_()\n",
    "for epoch in xrange(no_epochs):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    train_iter=iter(train_loader)\n",
    "    for batch in xrange(train_batches):\n",
    "        context_idxs=[]\n",
    "        cats=[]\n",
    "        # Step 1. Prepare the inputs to be passed to the model.  To do this we will:\n",
    "        #  iterate around each sentance in the batch creating a numerical array of the \n",
    "        #  word indecies\n",
    "        inputs,targets=train_iter.next()\n",
    "        t_inputs=autograd.Variable(inputs)\n",
    "        t_targets=autograd.Variable(targets)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(t_inputs)\n",
    "        _,ind=log_probs.data.topk(3,1,True,True)\n",
    "        if epoch==no_epochs-1:\n",
    "            predict[batch*batch_size:(batch+1)*batch_size,:]=ind\n",
    "            all_targets[batch*batch_size:(batch+1)*batch_size]=targets\n",
    "        loss=loss_function(log_probs, t_targets)\n",
    "        #print \"Batch: {0}, Loss= {1}\".format(batch,loss.data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.data\n",
    "    if epoch % 25==0:\n",
    "        print \"Epoch: {0}, Loss= {1}\".format(epoch,total_loss.numpy()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "print \"Training error: \", total_loss\n",
    "train_res=accuracy(predict,all_targets,topk=(1,3))\n",
    "print \"Training accuracy: \", train_res   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(len(label_to_idx)))\n",
    "class_total = list(0. for i in range(len(label_to_idx)))\n",
    "for i in range(len(predict)):\n",
    "    c = (predict[i][0] == all_targets[i])\n",
    "    label = all_targets[i]\n",
    "    class_correct[label] += c\n",
    "    class_total[label] += 1\n",
    "\n",
    "for i in range(len(label_to_idx)):\n",
    "    #print 'Accuracy of {0}'.format(label_to_idx[0])\n",
    "    if class_total[i]>0:\n",
    "        acc=(100 * class_correct[i] / class_total[i])\n",
    "    else:\n",
    "        acc=0\n",
    "    label=label_to_idx.keys()[label_to_idx.values().index(i)]    \n",
    "    print 'Accuracy of {0:20s} : {1:6.2f} % from a population of {2:3d}'.format(\n",
    "        label, acc, int(class_total[i]))\n",
    "print 'Total training cases: {0}, Vocab: {1}'.format(len(predict),vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntopk=3\n",
    "val_predict=torch.LongTensor(val_batches*batch_size,ntopk).zero_()\n",
    "val_targets=torch.LongTensor(val_batches*batch_size).zero_()\n",
    "val_total_loss = torch.Tensor([0])\n",
    "val_iter=iter(val_loader)\n",
    "v_total_loss=0\n",
    "for batch in xrange(val_batches):\n",
    "    context_idxs=[]\n",
    "    cats=[]\n",
    "    inputs,targets=val_iter.next()\n",
    "    model.eval()\n",
    "    #bp() # This is a breakpoint.\n",
    "    vinputs = autograd.Variable(inputs)\n",
    "    #targets=autograd.Variable(torch.LongTensor(np.asarray(cats,dtype='int64')).view(batch_size,-1))\n",
    "    vtargets=autograd.Variable(targets)\n",
    "    log_probs = model(vinputs)\n",
    "    _,ind=log_probs.data.topk(3,1,True,True)\n",
    "    val_predict[batch*batch_size:(batch+1)*batch_size,:]=ind\n",
    "    val_targets[batch*batch_size:(batch+1)*batch_size]=targets\n",
    "    loss=loss_function(log_probs, vtargets)\n",
    "    #print \"Batch: {0}, Loss= {1}\".format(batch,loss.data)\n",
    "    v_total_loss+=loss.data\n",
    "print \"Validation total loss, Loss= {0}\".format(v_total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " # Test accuracy\n",
    "test_res=accuracy(val_predict,val_targets,topk=(1,3))\n",
    "print \"Test accuracy: \", test_res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(len(label_to_idx)))\n",
    "class_total = list(0. for i in range(len(label_to_idx)))\n",
    "for i in range(len(val_predict)):\n",
    "    c = (val_predict[i][0] == val_targets[i])\n",
    "    label = val_targets[i]\n",
    "    class_correct[label] += c\n",
    "    class_total[label] += 1\n",
    "\n",
    "for i in range(len(label_to_idx)):\n",
    "    #print 'Accuracy of {0}'.format(label_to_idx[0])\n",
    "    if class_total[i]>0:\n",
    "        acc=(100 * class_correct[i] / class_total[i])\n",
    "    else:\n",
    "        acc=0\n",
    "    label=label_to_idx.keys()[label_to_idx.values().index(i)]\n",
    "    print 'Accuracy of {0:20s} : {1:6.2f} % from a population of {2:3d}'.format(\n",
    "        label, acc, int(class_total[i]))\n",
    "print 'Total validation cases: {0}'.format(len(val_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
